{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430c9871",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a method used to explain the output of machine learning models.  \n",
    "SHAP aims to explain how an input affects the output of the model, by showing the impact of each input feature on the output.  \n",
    "When reading the SHAP values, you will see for each input feature how much it positively or negatively pushed the output to the answer we got, compared to the average base value of the dataset.\n",
    "\n",
    "You can read more here: https://trustyai-explainability.github.io/trustyai-site/main/local-explainers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"onnx\" \"onnxruntime\" \"numpy==1.26.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe95ffca-bec7-4d0b-b976-80df5ddc0463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd540f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129f634",
   "metadata": {},
   "source": [
    "Let's start by loading some artifacts.  \n",
    "We will need:\n",
    "- The ONNX model\n",
    "- Our pre-and-post processing artifacts\n",
    "    - scaler.pkl\n",
    "    - label_encoder.pkl\n",
    "- Some data\n",
    "    - The training inputs, these will be used to get an average input for our dataset\n",
    "    - The test data, these will be used to get a point we want to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9845c-f417-4d5d-88c2-0aca52b3da25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_session = rt.InferenceSession(\"onnx_model.onnx\", providers=rt.get_available_providers())\n",
    "onnx_input_name = onnx_session.get_inputs()[0].name\n",
    "onnx_output_name = onnx_session.get_outputs()[0].name\n",
    "\n",
    "\n",
    "with open('scaler.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "\n",
    "with open('label_encoder.pkl', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)\n",
    "\n",
    "with open('train_data.pkl', 'rb') as handle:\n",
    "    train_data = pickle.load(handle)\n",
    "\n",
    "with open('test_data.pkl', 'rb') as handle:\n",
    "    test_data = pickle.load(handle)\n",
    "\n",
    "X_test = test_data[0]\n",
    "X_train_scaled = pd.DataFrame(data=train_data[0], columns=X_test.columns) #The X_train is scaled in the pre-processing step in the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d92ab",
   "metadata": {},
   "source": [
    "We arbitrarily choose the first data point (song) in our test data to be the data we want to test.  \n",
    "In practice, you might choose the data point that you predict the worst on, or a data point that gave an unexpected answer.  \n",
    "We also look at how our data point looks when normalized (after going through pre-processing). This is how it will look like going into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55a6be-f889-4a35-9993-8d5111a830e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "point_to_explain = X_test.iloc[0:1]\n",
    "point_to_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ed0e3e-865e-4cb3-8e69-039e9b72217a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    normalized_data = scaler.transform(df)\n",
    "    return pd.DataFrame(normalized_data, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f245332",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_dataframe(point_to_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fb52e",
   "metadata": {},
   "source": [
    "We grab all the country codes from the post-processing artifact label_encoder.  \n",
    "We will use these to know what output represents what country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd30546-484f-487c-9aa1-c532d16df6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_names = label_encoder.classes_\n",
    "output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e29b8f",
   "metadata": {},
   "source": [
    "TrustyAI SHAP explainer requires our model to have a pandas dataframe as an input, and numpy or pandas output, so we wrap our model in a pred() function that makes sure the input and output are converted properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0df053e9-bf56-4b03-9e9c-f7876cb19269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    pred = onnx_session.run([onnx_output_name], {onnx_input_name: x.to_numpy().astype(np.float32)})[0]\n",
    "    return pd.DataFrame(pred, columns=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f587512-38e8-4e80-b78d-f6296d573924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustyai.model import Model\n",
    "trustyai_model = Model(pred, dataframe_input=True, output_names=output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e713b40",
   "metadata": {},
   "source": [
    "Let's try to use our TrustyAI Model to predict the output of our data point we want to explain with SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc63b48-8d61-4028-bdbd-219217e4f5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = trustyai_model(normalize_dataframe(point_to_explain))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38c14c",
   "metadata": {},
   "source": [
    "And with everything set up, we can create a SHAP explainer and let it analyze our data point!  \n",
    "You can also note that we add 100 data points from our training dataset to the SHAPExplainer, this is used to calculate the average base values of our dataset. With this, we can see how much our interesting datapoint contributes to the prediction compared to what a \"standard\" value would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "165978b5-2c24-4043-8757-f8944e815695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustyai.explainers import SHAPExplainer\n",
    "explainer = SHAPExplainer(background=X_train_scaled[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8996696-e26e-4eca-a907-6e62c033f9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explanations = explainer.explain(inputs=normalize_dataframe(point_to_explain),\n",
    "                                 outputs=prediction,\n",
    "                                 model=trustyai_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2e45e",
   "metadata": {},
   "source": [
    "With our SHAP Explainer ready we can start looking at the results.\n",
    "\n",
    "Let's choose a specific output country which we want to know how it got affected by the input values.  \n",
    "CH is the country that we are supposed to get as the popular country for this input, so it's especially interesting to see the input's effect on that output.  \n",
    "That being said, feel free to try with a few other countries and see what happens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebd777e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_OF_INTEREST = \"CH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30d71c",
   "metadata": {},
   "source": [
    "First, we will get a table of values.  \n",
    "Here we can see the **Mean Background Value** - this is the average base value we were talking about before.  \n",
    "We can also see our **Value**, which is the normalized data point that we sent into the explainer. Red values are lower than the average value and green values are higher.  \n",
    "Finally, we have the **SHAP Value**. These indicate how much that input feature had an effect on the output. Red indicates a negative contribution to the prediction while green a positive contribution. The larger the value, the larger the contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e764b-4412-4c47-9516-b38eae9c0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations.as_html()[COUNTRY_OF_INTEREST]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c53c38",
   "metadata": {},
   "source": [
    "We can also visualize it as a candlestick plot, seeing how the different input features build up to the output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustyai.visualizations.shap import SHAPViz\n",
    "SHAPViz()._matplotlib_plot(explanations=explanations, output_name=COUNTRY_OF_INTEREST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
