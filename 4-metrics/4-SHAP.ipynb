{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430c9871",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a method used to explain the output of machine learning models.  \n",
    "SHAP aims to explain how an input affects the output of the model, by showing the impact of each input feature on the output.  \n",
    "When reading the SHAP values, you will see for each input feature how much it possitively or negatively pushed the output to the answer we got, compared to a average base value of the dataset.\n",
    "\n",
    "You can read more here: https://trustyai-explainability.github.io/trustyai-site/main/local-explainers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"onnx\" \"onnxruntime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95ffca-bec7-4d0b-b976-80df5ddc0463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129f634",
   "metadata": {},
   "source": [
    "Let's start by loading some artifacts.  \n",
    "We will need:\n",
    "- The ONNX model\n",
    "- Our pre-and-post processing artifacts\n",
    "    - scaler.pkl\n",
    "    - label_encoder.pkl\n",
    "- Some data\n",
    "    - The training inputs, these will be used to get an average input for our dataset\n",
    "    - The test data, these will be used to get a point we want to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9845c-f417-4d5d-88c2-0aca52b3da25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_session = rt.InferenceSession(\"../2-dev_datascience/models/jukebox/1/model.onnx\", providers=rt.get_available_providers())\n",
    "onnx_input_name = onnx_session.get_inputs()[0].name\n",
    "onnx_output_name = onnx_session.get_outputs()[0].name\n",
    "\n",
    "\n",
    "with open('../2-dev_datascience/models/jukebox/1/artifacts/scaler.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "\n",
    "with open('../2-dev_datascience/models/jukebox/1/artifacts/label_encoder.pkl', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)\n",
    "\n",
    "with open('../2-dev_datascience/models/jukebox/1/artifacts/y_test.pkl', 'rb') as handle:\n",
    "    y_test = pickle.load(handle)\n",
    "\n",
    "X_train = pd.read_parquet(\"../2-dev_datascience/models/jukebox/1/artifacts/X_train.parquet\")\n",
    "X_test = pd.read_parquet(\"../2-dev_datascience/models/jukebox/1/artifacts/X_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d92ab",
   "metadata": {},
   "source": [
    "We arbitrarily choose the first datapoint in our test data to be the data we want to test.  \n",
    "In practice, you might choose the datapoint that you predict the worst on, or a datapoint that gave an unexpected answer.  \n",
    "We also look at how our datapoint looks like when normalized (after going through pre-processing). This is how it will look like going into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55a6be-f889-4a35-9993-8d5111a830e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "point_to_explain = X_test.iloc[0:1]\n",
    "point_to_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed0e3e-865e-4cb3-8e69-039e9b72217a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    normalized_data = scaler.transform(df)\n",
    "    return pd.DataFrame(normalized_data, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f245332",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_dataframe(point_to_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fb52e",
   "metadata": {},
   "source": [
    "We grab all the countrycodes from the post-processing artifact label_encoder.  \n",
    "We will use these to know what output is what country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd30546-484f-487c-9aa1-c532d16df6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_names = label_encoder.classes_\n",
    "output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e29b8f",
   "metadata": {},
   "source": [
    "TrustyAI SHAP explainer requires our model to have a pandas dataframe as an input, and numpy or pandas output, so we wrap our model in a pred() function that makes sure the input and output are converted properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df053e9-bf56-4b03-9e9c-f7876cb19269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    pred = onnx_session.run([onnx_output_name], {onnx_input_name: x.to_numpy().astype(np.float32)})[0]\n",
    "    return pd.DataFrame(pred, columns=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f587512-38e8-4e80-b78d-f6296d573924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustyai.model import Model\n",
    "trustyai_model = Model(pred, dataframe_input=True, output_names=output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e713b40",
   "metadata": {},
   "source": [
    "Let's try to use our TrustyAI Model to predict the output of our point we want to explain with SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc63b48-8d61-4028-bdbd-219217e4f5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trustyai_model(normalize_dataframe(point_to_explain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38c14c",
   "metadata": {},
   "source": [
    "And with everything set up, we can create a SHAP explainer and let it analyze our datapoint!  \n",
    "You can also note that we add 100 datapoints from our training dataset to the SHAPExplainer, this is used to calculate the average base values of our dataset. With this we can see how much our interesting datapoint contributes to the prediction compared to what a \"standard\" value would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165978b5-2c24-4043-8757-f8944e815695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustyai.explainers import SHAPExplainer\n",
    "explainer = SHAPExplainer(background=normalize_dataframe(X_train[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8996696-e26e-4eca-a907-6e62c033f9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explanations = explainer.explain(inputs=normalize_dataframe(point_to_explain),\n",
    "                                 outputs=pd.DataFrame([dict(zip(output_names, y_test[0]))]), #This is just the ground truth of the point_to_explain\n",
    "                                 model=trustyai_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2e45e",
   "metadata": {},
   "source": [
    "With our SHAP Explainer ready we can start looking at the results.\n",
    "\n",
    "Let's choose a specific output country which we want to know how the input affected.  \n",
    "CH is the country which we are supposed to get as the popular country for this input, so it's especially interesting to see the inputs effect on that output.  \n",
    "That being said, feel free to try with a few other countries and see what happens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd777e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_OF_INTEREST = \"CH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30d71c",
   "metadata": {},
   "source": [
    "First we will get a table of values.  \n",
    "Here we can see the **Mean Background Value** - this is the average base value we were talking about before.  \n",
    "We can also see our **Value**, which is the normalized datapoint that we sent into the explainer. Red values are lower than the average value and green values are higher.  \n",
    "Finally, we have the **SHAP Value**. These indicate how much that input feature had an effect on the output. Red indicate a negative contribution to the prediction while green a possitive contribution. The large the value, the larger the contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e764b-4412-4c47-9516-b38eae9c0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations.as_html()[COUNTRY_OF_INTEREST]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c53c38",
   "metadata": {},
   "source": [
    "We can also visualize it as a candlestick plot, seeing how the different input features build up to the output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustyai.visualizations.shap import SHAPViz\n",
    "SHAPViz()._matplotlib_plot(explanations=explanations, output_name=COUNTRY_OF_INTEREST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
